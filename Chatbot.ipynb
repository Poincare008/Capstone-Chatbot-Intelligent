{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e10a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a5bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# load the dataset\n",
    "with open('data/healthcare.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "intents = {'intents':[]}\n",
    "#\n",
    "for convo in data:\n",
    "    tag = convo.get(\"agent_selected_tool\", \"general\").replace(\" \",\"_\").lower()\n",
    "\n",
    "\n",
    "    pattern = convo.get(\"user_1\", \"\")\n",
    "    response = convo.get(\"agent_initial_response\", \"\")\n",
    "\n",
    "\n",
    "    if pattern and response:\n",
    "        intents['intents'].append({\n",
    "            \"tag\":tag,\n",
    "            \"patterns\":[pattern],\n",
    "            \"responses\":[response]\n",
    "        })\n",
    "\n",
    "# Save the new dataset\n",
    "with open(\"data/healthcare_intents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(intents, f, indent=4)\n",
    "\n",
    "\n",
    "print('Dataset intentions create with sucess!')\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class ChatboModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(128,output_size)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class ChabotAssistant:\n",
    "    def __init__(self,intents_path, function_mappings = None):\n",
    "        self.model = None\n",
    "        self.intents_path = intents_path\n",
    "\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = {}\n",
    "\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        #\n",
    "        for resource in ['wordnet', 'omw-1.4', 'punkt']:\n",
    "            try:\n",
    "                nltk.data.find(f'corpora/{resource}')\n",
    "            except LookupError:\n",
    "                 nltk.download(resource)\n",
    "\n",
    "    #\n",
    "    @staticmethod\n",
    "    def tokenize_and_lemmatize(text):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def bag_of_words(self, words):\n",
    "        return[1 if word in words else 0 for word in self.vocabulary]\n",
    "    \n",
    "    #\n",
    "    def parse_intents(self):\n",
    "        #lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        if os.path.exists(self.intents_path):\n",
    "            with open(self.intents_path,'r', encoding= \"utf-8\") as f:\n",
    "                intents_data = json.load(f)\n",
    "\n",
    "            for intent in intents_data['intents']:\n",
    "                if intent['tag'] not in self.intents:\n",
    "                    self.intents.append(intent['tag'])\n",
    "                    self.intents_responses[intent['tag']] = intent['responses']\n",
    "                \n",
    "                for pattern in intent['patterns']:\n",
    "                    pattern_words = self.tokenize_and_lemmatize(pattern)\n",
    "                    self.vocabulary.extend(pattern_words)\n",
    "                    self.documents.append((pattern_words, intent['tag']))\n",
    "                \n",
    "            self.vocabulary = sorted(set(self.vocabulary))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"???{self.intents_path}???\")\n",
    "\n",
    "    #\n",
    "    def prepare_data(self):\n",
    "        bags = []\n",
    "        indices = []\n",
    "\n",
    "        for words, intent in self.documents:\n",
    "            bag = self.bag_of_words(words)\n",
    "            intent_index = self.intents.index(intent)\n",
    "            bags.append(bag)\n",
    "            indices.append(intent_index)\n",
    "\n",
    "            \n",
    "\n",
    "        self.X = np.array(bags)\n",
    "        self.y = np.array(indices)\n",
    "\n",
    "        print(f\"Prepared {len(self.X)} training examples.\")\n",
    "\n",
    "    #\n",
    "    def train_model(self, batch_size, lr, epochs):\n",
    "        X_tensor = torch.tensor(self.X,dtype = torch.float32)\n",
    "        y_tensor = torch.tensor(self.y,dtype = torch.long)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor,y_tensor)\n",
    "        loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.model = ChatboModel(self.X.shape[1],len(self.intents))\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr = lr)\n",
    "\n",
    "        loss_values = []\n",
    "\n",
    "        print(\"\\n Starting training...\\n\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in tqdm(loader,desc= f\"Epoch {epoch+1}/{epochs}\",leave=False):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(loader)\n",
    "            loss_values.append(epoch_loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(loss_values, label= \"Training Loss\", color = \"blue\")\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show\n",
    "\n",
    "    #\n",
    "    def save_model(self,model_path, dimensions_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        with open(dimensions_path,\"w\") as f:\n",
    "            json.dump({'input_size': self.X.shape[1],'output_size':len(self.intents)},f, indent=4)\n",
    "\n",
    "    def load_model(self,model_path, dimensions_path):\n",
    "        with open(dimensions_path,\"r\") as f:\n",
    "            dimensions = json.load(f)\n",
    "\n",
    "        self.model = ChatboModel(dimensions['input_size'], dimensions['output_size'])\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    #\n",
    "    def process_message(self, input_message):\n",
    "        words = self.tokenize_and_lemmatize(input_message)\n",
    "        bag = self.bag_of_words(words)\n",
    "        bag_tensor = torch.tensor([bag], dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(bag_tensor)\n",
    "\n",
    "        predicted_class_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_intent = self.intents[predicted_class_index]\n",
    "\n",
    "        if self.function_mappings and predicted_intent in self.function_mappings:\n",
    "            self.function_mappings[predicted_intent]()\n",
    "\n",
    "        if self.intents_responses[predicted_intent]:\n",
    "            return random.choice(self.intents_responses[predicted_intent])\n",
    "        else:\n",
    "            return \"?????.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a1edac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 683 training examples.\n",
      "Chatbot: I can look into that for you. Can you provide your name and the claim number?\n",
      "Chatbot: Certainly. Which department are you trying to reach?\n",
      "Chatbot: Our clinic operates Monday through Saturday.\n",
      "Chatbot: It’s best to book an appointment so a doctor can evaluate your symptoms.\n",
      "Chatbot: I’m not a doctor, but I can help you understand possible causes. How long have you been feeling this way?\n",
      "Chatbot: Sure! Can you please provide your name and preferred date and time?\n",
      "Chatbot: Goodbye! \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "if __name__== '__main__':\n",
    "    assistant = ChabotAssistant(\"data/healthcare_intents.json\")\n",
    "    assistant.parse_intents()\n",
    "    assistant.prepare_data()\n",
    "    #assistant.train_model(batch_size=32, lr =0.001, epochs=150)    \n",
    "\n",
    "    #assistant.save_model('chat_model.pth','dimensions.json')\n",
    "\n",
    "\n",
    "    assistant.load_model('chat_model.pth','dimensions.json')\n",
    "\n",
    "    while True:\n",
    "        message = input(\"You: \")\n",
    "        if message.lower() == \"/quit\":\n",
    "            print(\"Chatbot: Goodbye! \")\n",
    "            break\n",
    "        print(\"Chatbot:\", assistant.process_message(message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b8fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1865ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b934e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123927ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0a83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14983b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9977f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
