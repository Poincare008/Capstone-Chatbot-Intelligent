{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e10a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a5bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a56ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset intentions create with sucess!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# load the dataset\n",
    "with open('data/healthcare.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "intents = {'intents':[]}\n",
    "#\n",
    "for convo in data:\n",
    "    tag = convo.get(\"agent_selected_tool\", \"general\").replace(\" \",\"_\").lower()\n",
    "\n",
    "\n",
    "    pattern = convo.get(\"user_1\", \"\")\n",
    "    response = convo.get(\"agent_initial_response\", \"\")\n",
    "\n",
    "\n",
    "    if pattern and response:\n",
    "        intents['intents'].append({\n",
    "            \"tag\":tag,\n",
    "            \"patterns\":[pattern],\n",
    "            \"responses\":[response]\n",
    "        })\n",
    "\n",
    "# Save the new dataset\n",
    "with open(\"data/healthcare_intents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(intents, f, indent=4)\n",
    "\n",
    "\n",
    "print('Dataset intentions create with sucess!')\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db884478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Calixte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "for resource in ['wordnet', 'omw-1.4', 'punkt']:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{resource}')\n",
    "    except LookupError:\n",
    "        nltk.download(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36a16dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (vocabulaire): 990\n",
      "Output size (intentions): 137\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "with open(\"data/healthcare_intents.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Vocabulaire\n",
    "vocab = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        vocab.extend([lemmatizer.lemmatize(word.lower()) for word in tokens])\n",
    "\n",
    "vocab = sorted(set(vocab))\n",
    "input_size = len(vocab)\n",
    "\n",
    "# Intentions\n",
    "intents = [intent[\"tag\"] for intent in data[\"intents\"]]\n",
    "output_size = len(set(intents))\n",
    "\n",
    "print(f\"Input size (vocabulaire): {input_size}\")\n",
    "print(f\"Output size (intentions): {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatboModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(128,output_size)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChabotAssistant:\n",
    "    def __init__(self,intents_path, function_mappings = None):\n",
    "        self.model = None\n",
    "        self.intents_path = intents_path\n",
    "\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = []\n",
    "\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_and_lemmatize(text):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(words.lower()) for word in words]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def bag_of_words(self, words):\n",
    "        return[1 if word in words else 0 for word in self.vocabulary]\n",
    "    \n",
    "    def parse_intents(self):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        if os.path.exits(self.intents_path):\n",
    "            with open(self.intents_path,'r') as f:\n",
    "                intents_data = json.load(f)\n",
    "\n",
    "            for intent in intents_data['intents']:\n",
    "                if intent['tag'] not in self.intents:\n",
    "                    self.intents.append(intent['tag'])\n",
    "                    self.intents_responses[intent['tag']] = intent['responses']\n",
    "                \n",
    "                for pattern in intent['patterns']:\n",
    "                    pattern_words = self.tokenize_and_lemmatize(pattern)\n",
    "                    self.vocabulary.extend(pattern_words)\n",
    "                    self.documents.append((pattern_words, intent['tag']))\n",
    "                \n",
    "                self.vocabulary = sorted(set(self.vocabulary))\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        bags = []\n",
    "        indices = []\n",
    "\n",
    "        for document in self.documents:\n",
    "            words = document[0]\n",
    "            bag = self.bag_of_words(words)\n",
    "\n",
    "            intent_index = self.index(document[1])\n",
    "\n",
    "            bags.append(bag)\n",
    "            indices.append(intent_index(document[1]))\n",
    "\n",
    "            bags.append(bags)\n",
    "            indices.append(intent_index)\n",
    "\n",
    "        self.X = np.array(bags)\n",
    "        self.y = np.array(indices)\n",
    "\n",
    "\n",
    "    def train_model(self, batch_size, lr, epochs):\n",
    "        X_tensor = torch.tensor(self.X,dtype = torch.float32)\n",
    "        y_tensor = torch.tensor(self.y,dtype = torch.long)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor,y_tensor)\n",
    "        loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.model = ChatboModel(self.X.shape[1],len(self.intents))\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Loss: {running_loss/len(loader):.4f}\")\n",
    "\n",
    "    \n",
    "    def save_model(self,model_path, dimensions_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        with open(dimensions_path,\"r\") as f:\n",
    "            json.dump({'input_size': self.X.shape[1],'output_size':len(self.intents)},f)\n",
    "\n",
    "    def load_model(self,model_path, dimensions_path):\n",
    "        with open(dimensions_path,\"r\") as f:\n",
    "            dimensions = json.load(f)\n",
    "\n",
    "        self.model = ChatboModel(dimensions['input_size'], dimensions['output_size'])\n",
    "        self.model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "    def process_message(self, input_message):\n",
    "        words = self.tokenize_and_lemmatize(input_message)\n",
    "        bag = self.bag_of_words(words)\n",
    "\n",
    "        bag_tensor = torch.tensor([bag], dtype=torch.float32)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(bag_tensor)\n",
    "\n",
    "        predicted_class_index = torch.argmax(predictions, dim=1).item()\n",
    "        predicted_intent = self.intents[predicted_class_index]\n",
    "\n",
    "        if self.function_mappings:\n",
    "            if predicted_intent in self.function_mappings:\n",
    "                self.function_mappings[predicted_intent]()\n",
    "        \n",
    "        if self.intents_responses[predicted_intent]:\n",
    "            return random.choice(self.intents_responses[predicted_intent])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== '__main__':\n",
    "    assistant = ChabotAssistant()\n",
    "    assistant.parse_intents()\n",
    "    assistant.prepare_data()\n",
    "    assistant.train_model(batch_size=32, lr =0.001, epochs=150)    \n",
    "\n",
    "    assistant.save_model('chat_model.pth','dimensions.json')\n",
    "\n",
    "\n",
    "    #assistant = assistant.load_model('chat_model.pth','dimensions.json')\n",
    "\n",
    "    while True:\n",
    "        message = input('Enter your message:')\n",
    "\n",
    "        if message == '/quit':\n",
    "            break\n",
    "\n",
    "        print(assistant.process_message(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b8fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1865ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b934e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123927ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0a83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14983b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9977f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
